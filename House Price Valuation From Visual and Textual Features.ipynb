{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "wHAavriP3MRH",
        "zdRf9fHA3RR1"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AmirhosseinSalamirad/House-Price-Valuation/blob/main/HousePrice_NewApproach_emadHamed.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#House Price Valuation From Visual and Textual Features\n",
        "This code is developed to test the methodology presented in *Ahmed, E., & Moustafa, M. (2016). House price estimation from visual and textual features. arXiv preprint arXiv:1609.08399.*"
      ],
      "metadata": {
        "id": "_73W9KJViO61"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QwDekcq9juPg"
      },
      "source": [
        "import json, re, os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn import neighbors\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, make_scorer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder, label_binarize,MinMaxScaler, RobustScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.ensemble import GradientBoostingRegressor\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "tZZTJRuFiOEf"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iCLjh2IwMvJN",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 105
        },
        "outputId": "f7dc0854-3e03-45cb-f510-fcb8d02c3292"
      },
      "source": [
        "!sudo python3 -m pip install opencv-python==3.4.2.16\n",
        "!sudo python3 -m pip install opencv-contrib-python==3.4.2.16"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: opencv-python==3.4.2.16 in /usr/local/lib/python3.6/dist-packages (3.4.2.16)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-python==3.4.2.16) (1.18.2)\n",
            "Requirement already satisfied: opencv-contrib-python==3.4.2.16 in /usr/local/lib/python3.6/dist-packages (3.4.2.16)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.6/dist-packages (from opencv-contrib-python==3.4.2.16) (1.18.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZHoEnQuNJEI"
      },
      "source": [
        "# Testing old paper approach"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3XKHucJNRfn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "c76f82c1-804e-49ab-a1a3-b144c06822d6"
      },
      "source": [
        "!git clone https://github.com/emanhamed/Houses-dataset.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Houses-dataset'...\n",
            "remote: Enumerating objects: 2166, done.\u001b[K\n",
            "remote: Total 2166 (delta 0), reused 0 (delta 0), pack-reused 2166\u001b[K\n",
            "Receiving objects: 100% (2166/2166), 176.26 MiB | 24.02 MiB/s, done.\n",
            "Resolving deltas: 100% (20/20), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGo2lALmxFc0"
      },
      "source": [
        "# import cv2\n",
        "# import os\n",
        "# import numpy as np\n",
        "\n",
        "# rootdir = \"/content/Houses-dataset/Houses Dataset/\"\n",
        "# surf = cv2.xfeatures2d.SURF_create()\n",
        "# final_results = []\n",
        "\n",
        "# img_types = ['bathroom', 'bedroom', 'frontal', 'kitchen']\n",
        "# for img_idx in range(1, 21):#536):\n",
        "# \ttemp_results = []\n",
        "# \tfor img_type in img_types:\n",
        "# \t\timage = cv2.imread(rootdir + str(img_idx) + '_' + img_type + '.jpg')\n",
        "# \t\timage = image.astype('uint8')\n",
        "# \t\tgray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "# \t\t(kps, descs) = surf.detectAndCompute(gray, None)\n",
        "# \t\ttemp_results.append(descs[:4, :].flatten())\n",
        "# \tfinal_results.append(temp_results)\n",
        "# \tprint(img_idx)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-pvmmL7UUVu"
      },
      "source": [
        "# final_results = np.array(final_results)\n",
        "# final_results = final_results.reshape(20,-1)\n",
        "# final_results.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T7nKp-iQzfKh"
      },
      "source": [
        "house_data = pd.read_csv('HousesInfo.csv')\n",
        "SURF_features = pd.read_csv('SURF_features.csv')\n",
        "print(house_data.shape)\n",
        "print(SURF_features.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oW0l5-nOWOtD"
      },
      "source": [
        "a = pd.concat([SURF_features,house_data],1)\n",
        "a.shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XR8tCP1CcabJ"
      },
      "source": [
        "def train_model(dataset, base_model, params=None, verbose=0):\n",
        "\n",
        "    numeric_features = ['num_bedroom',\t'num_bathroom',\t'house_area','zipcode']\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler()),\n",
        "        # ('scaler', RobustScaler()),\n",
        "        # ('scaler', MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "                    ])\n",
        "\n",
        "    # Append classifier to preprocessing pipeline.\n",
        "    # Now we have a full prediction pipeline.\n",
        "    pipe = Pipeline(steps=[\n",
        "        # ('preprocessor', preprocessor),\n",
        "        ('regressor', base_model)],\n",
        "        verbose=False)\n",
        "\n",
        "    X = dataset.iloc[:, :-1]\n",
        "    y = dataset.iloc[:, -1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "    print('X_train.shape:\\t{}'.format(X_train.shape))\n",
        "    print('X_test.shape:\\t{}'.format(X_test.shape))\n",
        "\n",
        "    if params:\n",
        "        gs = GridSearchCV(pipe, params, cv=3, iid=False, n_jobs=-1, verbose=verbose)\n",
        "        gs.fit(X_train, y_train)\n",
        "        print(\"model score: {:.3}\".format(gs.score(X_test, y_test)))\n",
        "        print('Best Params:{}'.format(gs.best_params_))\n",
        "        print('Best Score:{:.3}'.format(gs.best_score_))\n",
        "        y_pred = gs.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        print('MSE = \\t{:.4}'.format(mse))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        print('MAE = \\t{:.4}'.format(mae))\n",
        "        cv_score = cross_val_score(pipe, X_test, y_test, scoring=make_scorer(mean_squared_error), cv=5)\n",
        "        print('cv_scores = \\t{}'.format(cv_score))\n",
        "        print(\"CV MSE =\\t{:.4} (+/- {:.3})\".format(cv_score.mean(), cv_score.std() * 2))\n",
        "        return gs\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print('MSE = \\t{:.4}'.format(mse))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print('MAE = \\t{:.4}'.format(mae))\n",
        "    cv_score = cross_val_score(pipe, X_test, y_test, cv=5)\n",
        "    print('cv_score = \\t{}'.format(cv_score))\n",
        "    print(\"CV MSE =\\t{:.4} (+/- {:.3})\".format(cv_score.mean(), cv_score.std() * 2))\n",
        "\n",
        "    return pipe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YaqGySHQZ-gm"
      },
      "source": [
        "%%time\n",
        "regressor = neighbors.KNeighborsRegressor()\n",
        "# params = None\n",
        "params = {'regressor__n_neighbors': list(range(15,25)), 'regressor__weights': ['uniform', 'distance']}\n",
        "# Best Params:{'regressor__n_neighbors': 24, 'regressor__weights': 'distance'}\n",
        "train_model(a, regressor, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kd9fqwaEtgja"
      },
      "source": [
        "%%time\n",
        "regressor = GradientBoostingRegressor()\n",
        "# regressor = GradientBoostingRegressor(\n",
        "#         learning_rate = 0.1,\n",
        "#         max_depth = 6,\n",
        "#         max_features = 0.3,\n",
        "#         min_samples_leaf = 9,\n",
        "#         n_estimators = 100\n",
        "#     )\n",
        "# params = None\n",
        "# Best Params:{'regressor__learning_rate': 0.1, 'regressor__max_depth': 6, 'regressor__max_features': 0.3, 'regressor__min_samples_leaf': 3, 'regressor__n_estimators': 100}\n",
        "params = {\n",
        "            'regressor__n_estimators':[100],\n",
        "            'regressor__learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
        "            'regressor__max_depth':[4,6],\n",
        "            'regressor__min_samples_leaf':[3,5,9,17],\n",
        "            'regressor__max_features':[1.0,0.3,0.1]\n",
        "        }\n",
        "\n",
        "train_model(a, regressor, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gvXyt_ty4TIl"
      },
      "source": [
        "# X = house_data.iloc[:, :-1]\n",
        "y = a.iloc[:, -1]\n",
        "\n",
        "target_scaler = MinMaxScaler()\n",
        "q = target_scaler.fit_transform(y[:, np.newaxis])\n",
        "a['sold_price'] = q"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CAcwAlKSumXy"
      },
      "source": [
        "a['sold_price'].describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QcpO9k1eBlUT"
      },
      "source": [
        "# from scipy import stats\n",
        "# import numpy as np\n",
        "\n",
        "# q = house_data\n",
        "# print('house_data.shape before removal: {}'.format(house_data.shape))\n",
        "\n",
        "# for i in range(15):\n",
        "#     z = np.abs(stats.zscore(q['sold_price']))\n",
        "#     q = q[(z < 3)]\n",
        "#     # print(q.shape)\n",
        "\n",
        "# house_data = q\n",
        "# print('house_data.shape after removal: {}'.format(house_data.shape))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gL1aAW289SLq"
      },
      "source": [
        "# q_scaler = MinMaxScaler()\n",
        "# q = q_scaler.fit_transform(house_data)\n",
        "# house_data = pd.DataFrame(q)\n",
        "# house_data.describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rfzh5IUqudnh"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHAavriP3MRH"
      },
      "source": [
        "# Training Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5PtjxeD0z_Oo"
      },
      "source": [
        "def train_model(dataset, base_model, params=None, verbose=0):\n",
        "\n",
        "    numeric_features = ['bedrooms', 'bathrooms', 'area',]\n",
        "    numeric_transformer = Pipeline(steps=[\n",
        "        ('scaler', StandardScaler()),\n",
        "        # ('scaler', RobustScaler()),\n",
        "        # ('scaler', MinMaxScaler())\n",
        "    ])\n",
        "\n",
        "    preprocessor = ColumnTransformer(\n",
        "        transformers=[\n",
        "            ('num', numeric_transformer, numeric_features),\n",
        "                    ])\n",
        "\n",
        "    # Append classifier to preprocessing pipeline.\n",
        "    # Now we have a full prediction pipeline.\n",
        "    pipe = Pipeline(steps=[\n",
        "        # ('preprocessor', preprocessor),\n",
        "        ('regressor', base_model)],\n",
        "        verbose=False)\n",
        "\n",
        "    X = dataset.iloc[:, :-1]\n",
        "    y = dataset.iloc[:, -1]\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\n",
        "    print('X_train.shape:\\t{}'.format(X_train.shape))\n",
        "    print('X_test.shape:\\t{}'.format(X_test.shape))\n",
        "\n",
        "    if params:\n",
        "        gs = GridSearchCV(pipe, params, cv=3, iid=False, n_jobs=-1, verbose=verbose)\n",
        "        gs.fit(X_train, y_train)\n",
        "        print(\"model score: {:.3}\".format(gs.score(X_test, y_test)))\n",
        "        print('Best Params:{}'.format(gs.best_params_))\n",
        "        print('Best Score:{:.3}'.format(gs.best_score_))\n",
        "        y_pred = gs.predict(X_test)\n",
        "        mse = mean_squared_error(y_test, y_pred)\n",
        "        print('MSE = \\t{:.4}'.format(mse))\n",
        "        mae = mean_absolute_error(y_test, y_pred)\n",
        "        print('MAE = \\t{:.4}'.format(mae))\n",
        "        cv_score = cross_val_score(pipe, X_test, y_test, scoring=make_scorer(mean_squared_error), cv=5)\n",
        "        print('cv_scores = \\t{}'.format(cv_score))\n",
        "        print(\"CV MSE =\\t{:.4} (+/- {:.3})\".format(cv_score.mean(), cv_score.std() * 2))\n",
        "        return gs\n",
        "\n",
        "    pipe.fit(X_train, y_train)\n",
        "    y_pred = pipe.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print('MSE = \\t{:.4}'.format(mse))\n",
        "    mae = mean_absolute_error(y_test, y_pred)\n",
        "    print('MAE = \\t{:.4}'.format(mae))\n",
        "    cv_score = cross_val_score(pipe, X_test, y_test, cv=5)\n",
        "    print('cv_score = \\t{}'.format(cv_score))\n",
        "    print(\"CV MSE =\\t{:.4} (+/- {:.3})\".format(cv_score.mean(), cv_score.std() * 2))\n",
        "\n",
        "    return pipe\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdRf9fHA3RR1"
      },
      "source": [
        "# Regressors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZxQraniB203W"
      },
      "source": [
        "## KNN\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XmzcJiKC0ZgN"
      },
      "source": [
        "%%time\n",
        "regressor = neighbors.KNeighborsRegressor()\n",
        "# params = None\n",
        "params = {'regressor__n_neighbors': list(range(15,25)), 'regressor__weights': ['uniform', 'distance']}\n",
        "# Best Params:{'regressor__n_neighbors': 24, 'regressor__weights': 'distance'}\n",
        "train_model(house_data, regressor, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pk3SOe4t3gmp"
      },
      "source": [
        "## XGBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQgpvAPa0oe9"
      },
      "source": [
        "%%time\n",
        "regressor = GradientBoostingRegressor()\n",
        "# regressor = GradientBoostingRegressor(\n",
        "#         learning_rate = 0.1,\n",
        "#         max_depth = 6,\n",
        "#         max_features = 0.3,\n",
        "#         min_samples_leaf = 9,\n",
        "#         n_estimators = 100\n",
        "#     )\n",
        "# params = None\n",
        "# Best Params:{'regressor__learning_rate': 0.1, 'regressor__max_depth': 6, 'regressor__max_features': 0.3, 'regressor__min_samples_leaf': 3, 'regressor__n_estimators': 100}\n",
        "params = {\n",
        "            'regressor__n_estimators':[100],\n",
        "            'regressor__learning_rate': [0.1, 0.05, 0.02, 0.01],\n",
        "            'regressor__max_depth':[4,6],\n",
        "            'regressor__min_samples_leaf':[3,5,9,17],\n",
        "            'regressor__max_features':[1.0,0.3,0.1]\n",
        "        }\n",
        "\n",
        "train_model(house_data, regressor, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uzai1mGY3mIF"
      },
      "source": [
        "## SVR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_na7ugxo0x_3"
      },
      "source": [
        "%%time\n",
        "# SVR\n",
        "regressor = SVR(gamma='auto')\n",
        "params = {'regressor__kernel': ['linear', 'poly', 'rbf'], 'regressor__C': [0.01, 0.1, 1, 5, 10]}\n",
        "# params = None\n",
        "train_model(house_data, regressor, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nk8dzx7VBRL3"
      },
      "source": [
        "## MLP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TDYIeYOjBSYG"
      },
      "source": [
        "%%time\n",
        "from sklearn.neural_network import MLPRegressor\n",
        "\n",
        "regressor = MLPRegressor()\n",
        "params = {'regressor__activation':['identity','logistic', 'tanh', 'relu']}\n",
        "train_model(house_data, regressor, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GAVwXCyLTOdk"
      },
      "source": [
        "## DTR"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qlwtoOVWBUrC"
      },
      "source": [
        "%%time\n",
        "regressor = DecisionTreeRegressor()\n",
        "params = {'regressor__criterion': ('mse', 'friedman_mse', 'mae'), 'regressor__splitter': ('best', 'random')}\n",
        "# params = None\n",
        "train_model(house_data, regressor, params)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fo2qZDDYTpjw"
      },
      "source": [
        "## AdaBoost"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SUYnzI6TUOe"
      },
      "source": [
        "from sklearn.ensemble import AdaBoostRegressor\n",
        "regressor = AdaBoostRegressor(GradientBoostingRegressor())\n",
        "# regressor = AdaBoostRegressor(base_estimator=GradientBoostingRegressor(\n",
        "#         learning_rate = 0.1,\n",
        "#         max_depth = 6,\n",
        "#         max_features = 0.3,\n",
        "#         min_samples_leaf = 9,\n",
        "#         n_estimators = 100\n",
        "#     ))\n",
        "params = {\n",
        "            'regressor__n_estimators':[50, 100],\n",
        "            'regressor__learning_rate': [1, 0.5, 0.02],\n",
        "            'regressor__loss':['linear', 'square', 'exponential'],\n",
        "        }\n",
        "train_model(house_data, regressor, params)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
